{"backend_state":"spawning","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-aedf9c56-fb5d-452e-b43d-83383ef5fcea.json","kernel":"julia-1.7","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"file_extension":".jl","mimetype":"application/julia","name":"julia","version":"1.7.2"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":1,"id":"930d1b","input":"function hlower(number)\n    guess = 50\n    iteration = 1\n        while(guess != number)\n            if(guess> number)\n                guess -= round(50/(2^iteration))\n            elseif(guess< number)\n                guess += round(50/(2^iteration))\n            end\n            iteration+=1\n            println(guess)\n        end\nend","output":{"0":{"data":{"text/plain":"hlower (generic function with 1 method)"},"exec_count":1,"output_type":"execute_result"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"434530","input":"# answer will contain a 0,1,2 depending on whether the letter is grey, yellow or green. But for now assume that answer is all 0s.\nf =open(\"words.txt\")\nlines = readlines(f)\nclose(f)\n\nfunction probability(word,answer)\n    letters = split(word,\"\")\n    totalwords = 12972\n    ourwords=0\n    \n#   all grey\n    if answer[1]==0 && answer[2]==0 && answer[3] ==0 && answer[4] ==0 && answer[5] ==0\n        for i in lines\n            if occursin(letters[1],i) || occursin(letters[2],i) || occursin(letters[3],i) || occursin(letters[4],i) || occursin(letters[5],i)\n                ourwords+=1\n            end\n        end\n        ourwords = totalwords-ourwords\n    end\n    p = ourwords/totalwords\n    information = -log2(p)\n    println(information)\n    return p\nend","output":{"0":{"data":{"text/plain":"probability (generic function with 1 method)"},"exec_count":10,"output_type":"execute_result"}},"pos":40,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"f2457d","input":"word = \"slate\"\nanswer = [0,0,0,0,0]\nprint(probability(word,answer))\n# I =3.91\n# p = 0.0667","output":{"0":{"name":"stdout","output_type":"stream","text":"3.9065569859317195\n0.06668208448967006"}},"pos":41,"type":"cell"}
{"cell_type":"code","exec_count":116,"id":"83aeba","input":"prob(\"s\")","output":{"0":{"data":{"text/plain":"5-element Vector{Int64}:\n 1565\n   93\n  533\n  516\n 3958"},"exec_count":116,"output_type":"execute_result"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"85f30e","input":"# hlower(1)\n# println()\n# hlower(3)\n# println()\n# hlower(7)\n# println()\n# hlower(42)\n# println()\n# hlower(55)\n# println()\n# hlower(69)\n\nhlower(18)","output":{"0":{"name":"stdout","output_type":"stream","text":"25.0\n13.0\n19.0\n16.0\n18.0\n"}},"pos":12,"type":"cell"}
{"cell_type":"code","exec_count":125,"id":"ee843f","input":"arr =prob(\"e\")./12972\narr","output":{"0":{"data":{"text/plain":"5-element Vector{Float64}:\n 0.02335800185013876\n 0.12550107924761023\n 0.06799259944495838\n 0.179386370644465\n 0.11732963305581252"},"exec_count":125,"output_type":"execute_result"}},"pos":30,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"73a125","input":"#probability of an even coin is\np=0.5\n#Using the information formula we get\nI= -log2(p)\nprint(\"p(x)=\"*string(p))\nprint(\", Information=\"*string(I))","output":{"0":{"name":"stdout","output_type":"stream","text":"p(x)=0.5, Information=1.0"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":137,"id":"15781b","input":"# This is considered one of the best words\nword = \"slate\"\nanswer = [0,0,0,0,0]\nprintln(\"slate's all grey's probability is: \",probability(word,answer))\n\n# This is also pretty common word, before algorithms broke Wordle\nword = \"crane\"\nanswer = [0,0,0,0,0]\nprintln(\"crane's all grey's probability is: \",probability(word,answer))\n\n# This is an overated guess\nword = \"adieu\"\nanswer = [0,0,0,0,0]\nprintln(\"adieu's all grey's probability is: \",probability(word,answer))\n\n# This is general considered one of the worst starting words ever\nword = \"fuzzy\"\nanswer = [0,0,0,0,0]\nprintln(\"fuzzy's all grey's probability is: \",probability(word,answer))","output":{"0":{"name":"stdout","output_type":"stream","text":"slate's all grey's probability is: 0.06668208448967006\ncrane's all grey's probability is: 0.12164662349676225\nadieu's all grey's probability is: 0.078245451742214\nfuzzy's all grey's probability is: 0.6070767807585569\n"}},"pos":44,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"7b85be","input":"#probability of an fair dice is\np=1/6\n#Using the information formula we get\nI= -log2(p)\nprint(\"p(x)=\"*string(p))\nprint(\", Information=\"*string(I))","output":{"0":{"name":"stdout","output_type":"stream","text":"p(x)=0.16666666666666666, Information=2.584962500721156"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"5aa16c","input":"using Plots\nx = [0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\ny=[]\nfor i in x\n    append!(y,-log2(i))\nend\nplot(x,y,xaxis = (\"Probability\"),yaxis = (\"Information\"))","output":{"0":{"data":{"image/svg+xml":"d49adcca3d13fc93fae7a1c97c5136ad4b333587"},"exec_count":5,"output_type":"execute_result"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"981aea","input":"f =open(\"words.txt\")\nlines = readlines(f)\nclose(f)\nfunction prob(letter)\n    e = [0,0,0,0,0]\n        for i in lines\n            if (cmp(letter,string(i[1]))==0)\n                e[1]=e[1]+1\n            end\n            if (cmp(letter,string(i[2]))==0)\n                e[2]=e[2]+1\n            end\n            if (cmp(letter,string(i[3]))==0)\n                e[3]=e[3]+1\n            end\n            if (cmp(letter,string(i[4]))==0)\n                e[4]=e[4]+1\n            end\n            if (cmp(letter,string(i[5]))==0)\n                e[5]=e[5]+1\n            end\n    end\n    return e\nend","output":{"0":{"data":{"text/plain":"prob (generic function with 1 method)"},"exec_count":6,"output_type":"execute_result"}},"pos":28,"type":"cell"}
{"cell_type":"markdown","id":"0cb89f","input":"In the example of a fair coin we can expect to know the result in one guess, hence why the bit value is 1, but what about about a dice. Then we see","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"1305ff","input":"# What is Wordle?","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"139e31","input":" <img src=\"dist.jpg\" alt=\"drawing\" width=\"700\"/> ","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"1d825c","input":"## Probability vs. Information","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"1e1bff","input":"Feel free to try out some more words, they could even be made up words, they just have to five letters, just follow the format I used above. For this iteration of the answer array keep it all 0s, since we run the operations on the 243 differnt versions of the colors in the Python lecture. This entire lecture was mostly the theory behind how we can solve Wordle with simple agorithms instead of something like Machine learning or NLP. As a result we will take a look at the code, and how to make a Wordle solver and simulator.","pos":45,"type":"cell"}
{"cell_type":"markdown","id":"1e7c0b","input":"There is a caveat for double letters, in that if there are two of a letter in the answer and in your guess they will both light up. If there is only one in the mystery word, but two in your guess, then one letter will be yellow/green and the other grey.\n<img src=\"double.webp\" alt=\"drawing\" width=\"600\"/>","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"34d929","input":"Lets create a function that give us the information and probability of the word, so we can plug it into our entropy equation.","pos":39,"type":"cell"}
{"cell_type":"markdown","id":"3f164e","input":"Wordle is a fun 10 minute way to distract yourself from studying, but as programmers and mathematicians, surely we have to find a way to optimize and solve the puzzle. If we want to solve it lets consider a small game. Lets pick a number from 1 - 100. I can guaruntee that I will always be able to get it in 7 or less guesses. This Julia program below shows what I mean, it will show its guesses and you can see how by guessing higher and lower it will find its number.","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"40a318","input":"### Final project: Wordle and Information Theory by Raghav Maddukuri","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"435bf8","input":"We explored the information relation between probability and information, but lets say we wanted to make a best guess for a random event that has some probability distribution. All the examples we explored we knew the exact probability, but lets consider some *continuous random variables*, where instead we have a likely probability, like this picture below: ","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"45549f","input":"Wordle is an online game you can find here:\nhttps://www.nytimes.com/games/wordle/index.html.\nIt is a daily challenge where you have to use six guesses to figure out a five letter mystery word.","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"4f46fc","input":"The function I wrote at the top is only coded for the the most likely scenario, and to find the optimal word, we need to consider all scenarios of green grey and yellow letters. However, you can already see how some letters are far better than others","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"547446","input":"If you look at the shape of the graph above, you see its a lot flatter. This isn't a coincidence, because if you consider an event with equal probability, say the dice from before, then we would get 6 columns all of 0.1666 height. As such we want a word, where regardless of the guess it will always limit our pool by a good amount. As you can see above the word slate's most likely scenario where all the letters are grey, narrows down the pool greatly. This scenario happens only 6% of the time, and this gives us an information value of 3.91. ","pos":36.5,"type":"cell"}
{"cell_type":"markdown","id":"556d52","input":"In the above example, I made an array containing the probabilities of 'e' being in each space, but in essence we would do this for every letter in every position. We wouldn't have to do this for every Wordle calculation, instead keeping a list of probabilities that we can later reference. Here is a full chart showing the distribution of each letter and its position.","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"599682","input":"So if you take a look at the code cell below you will see that I divide the occurences of e by ~13000. This is pool of accepted gueses by wordle. If you want to take a look it is called words.txt in the project directory. However, it is very forgiving in what qualifies as a word. As 3blue1brown put it \"the words that would cause fights in Scrabble\". ","pos":29.5,"type":"cell"}
{"cell_type":"markdown","id":"71b025","input":"The formulas above show that Information and Probability are obviously linked, but lets try some real life examples to see the relation.","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"7acd03","input":"## Goal of this lecture","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"93f26b","input":"<img src=\"edist2.JPG\" alt=\"drawing\" width=\"500\"/>","pos":36,"type":"cell"}
{"cell_type":"markdown","id":"98db84","input":"How about instead of using something like \"WEARY\", we use an optimal word. If we look at our distribution above, we see that it would have an \"E\",\"A\",\"S\",\"T\" & \"L\". Know any words? \"SLATE\" is a pretty good anagram of these letters, and below we can see its probability distribution. ","pos":35,"type":"cell"}
{"cell_type":"markdown","id":"a66ba9","input":"<img src=\"wordle-graphic-example.jpg\" alt=\"drawing\" width=\"600\"/>","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"a76c86","input":"What I propose we do is too use each letter's usage in the total of 5 letter words. For every letter we can create a probability array containing its likely hood to be in each of the 5 letter spots. Then we can simply add the probabilities of each letter in our guess's likelihood to get the total probability of the word, and get the expected information for each word. Afterwards we can find the highest entropy word and use that to find the best guess. For example,according to the chart below \"e\" is in many words. If we guess \"e\" we effectively limit the pool of words\n\nHere is the list of all possible guesses for 5-letter words, that I used to generate a probability array: https://github.com/3b1b/videos/blob/master/_2022/wordle/daa/possible_words.txt.","pos":27,"type":"cell"}
{"cell_type":"markdown","id":"ad7ea0","input":"In this lecture we explore how information theory can help us solve and understand Wordle, and learn to apply those concepts to other games as well.","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"b11204","input":"Here we see that we need on average 2 and a half guesses. Essentially as our choices become more numerous, our guesses become worse. In other words probability and Information are inversely related, as shown by this graph.","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"b3aeda","input":"## Math 157: Intro to Mathematical Software\n## UC San Diego, Winter 2022","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"b6490c","input":"As you can see in the picture above, there is a bit more than randomly guessing words. Each letter in every word you guess will be one three colors for its position in the mystery word. Grey means its not there. Yellow means the letter is in a different position. Green means you got the letter and the position. ","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"bbb83b","input":"In this first example consider an even coin, and we need to guess which face landed.","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"cec2d1","input":"Understanding the logic behind these graphs is beyond this class, but essentially to find a probablity, we need to sum a region of probabilities rather than simply pick an choose one. So if we follow the picture above, the probability between 1 and 2, is the sum of each slice under the graph from 1 to 2. If we let $H(X)$ be the expected value of information and N be the the length of the slice, we can represent the expected value of information as the following function: $$H(X)=E=-\\sum_i^Np(x)\\times\\log_2(p(x))$$","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"d6d871","input":"## Probabilities of each letter","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"d70385","input":"# So How Do we Ruin it?","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"d9a27c","input":" <img src=\"csv.png\" alt=\"drawing\" width=\"500\"/>","pos":23,"type":"cell"}
{"cell_type":"markdown","id":"dd0c43","input":"Now we have a heuristic for the value of a variable. Essentially now we know the importance of each letter to us. However, now we need to find the probability of each letter.","pos":25,"type":"cell"}
{"cell_type":"markdown","id":"e29e90","input":"Using our entropy equation from above we can generate an average expected information value for each word. With this we can generate a best first guess. Then with the information we get from our first guess, we can figure out the probability of each secondary guess using a new probability since the total possibilities of words has shrunk.","pos":38,"type":"cell"}
{"cell_type":"markdown","id":"eb4051","input":"<img src=\"Edist.JPG\" alt=\"drawing\" width=\"500\"/>","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"eb48df","input":"So how does this function isolate a single number despite there being 100? Well I create buckets for the possible number to be in. Guessing higher or lower gives me just as information as the other, and as a result I can constantly half my possible pool of numbers. In Information theory the unit of measurement for halving our pool of guesses is called a bit. A singular bit halves our pool, two makes it a quarter and so on. Tying back to the number game, I can guess the mystery number with 7 bits of information. For this specific case, the number of partitions is 7, but what about a number 1-50, 1-200. With some math we can figure out the probability of guessing the right number. This gets us a probability formula using information, which is simply: $$p=(\\frac{1}{2})^I$$ With some clever rewriting we can get the same formula, but using the probability of the event. This is: $$I = -\\log_2(p)$$ \n","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"fa2dc0","input":"Now we have the probability of each letter in each position. The next step is to combine this information together to create a graph that contains the probability of all the combinations of whether a letter is there in the mystery word. Recall each letter can be not present, present, and present and in the right position. Below is the example of the probability distribution of a word like \"Weary\". We see there is a high likelihood that you will get all greys, and that will happen about 14% of the time. As you go down the graph, you the probability of more an more letters matching decreasing rapidly. As you can imagine given the rare nature of \"W\" and \"Y\" in the english langauge this is a very low proability, and knowing we aren'T using these letters isn't very informative.\n","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"fbe887","input":"Above I have coded the probability calculator for if the word has all greys. In the above example we see my favorite word \"slate\" has about a 6.6 percent of being grey, meaning that 94% of the time I will always get a hint about the mystery word.","pos":42,"type":"cell"}
{"id":0,"time":1647184413765,"type":"user"}
{"last_load":1647183781130,"type":"file"}